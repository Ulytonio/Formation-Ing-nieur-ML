{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0c282f",
   "metadata": {},
   "source": [
    "# Partie 1 SimCLR"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf6c7ec3",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous allons appliquer SimCLR à nos données. Pour cela, j'ai pré entrainer un modèle SimCLR sur les Stanfords Dogs grâce au repertoire git de l'article. Et dans un deuxième temps, j'utilise le SimCLR pré entrainer sur ImageNet de l'article que je vais fine tune sur les classes de chiens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e47d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3721d0",
   "metadata": {},
   "source": [
    "## Routine de SimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76ece9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The SimCLR Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific simclr governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Data preprocessing and augmentation.\"\"\"\n",
    "\n",
    "import functools\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "CROP_PROPORTION = 0.875  # Standard for ImageNet.\n",
    "\n",
    "\n",
    "def random_apply(func, p, x):\n",
    "  \"\"\"Randomly apply function func to x with probability p.\"\"\"\n",
    "  return tf.cond(\n",
    "      tf.less(\n",
    "          tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
    "          tf.cast(p, tf.float32)), lambda: func(x), lambda: x)\n",
    "\n",
    "\n",
    "def random_brightness(image, max_delta, impl='simclrv2'):\n",
    "  \"\"\"A multiplicative vs additive change of brightness.\"\"\"\n",
    "  if impl == 'simclrv2':\n",
    "    factor = tf.random.uniform([], tf.maximum(1.0 - max_delta, 0),\n",
    "                               1.0 + max_delta)\n",
    "    image = image * factor\n",
    "  elif impl == 'simclrv1':\n",
    "    image = tf.image.random_brightness(image, max_delta=max_delta)\n",
    "  else:\n",
    "    raise ValueError('Unknown impl {} for random brightness.'.format(impl))\n",
    "  return image\n",
    "\n",
    "\n",
    "def to_grayscale(image, keep_channels=True):\n",
    "  image = tf.image.rgb_to_grayscale(image)\n",
    "  if keep_channels:\n",
    "    image = tf.tile(image, [1, 1, 3])\n",
    "  return image\n",
    "\n",
    "\n",
    "def color_jitter(image, strength, random_order=True, impl='simclrv2'):\n",
    "  \"\"\"Distorts the color of the image.\n",
    "  Args:\n",
    "    image: The input image tensor.\n",
    "    strength: the floating number for the strength of the color augmentation.\n",
    "    random_order: A bool, specifying whether to randomize the jittering order.\n",
    "    impl: 'simclrv1' or 'simclrv2'.  Whether to use simclrv1 or simclrv2's\n",
    "        version of random brightness.\n",
    "  Returns:\n",
    "    The distorted image tensor.\n",
    "  \"\"\"\n",
    "  brightness = 0.8 * strength\n",
    "  contrast = 0.8 * strength\n",
    "  saturation = 0.8 * strength\n",
    "  hue = 0.2 * strength\n",
    "  if random_order:\n",
    "    return color_jitter_rand(\n",
    "        image, brightness, contrast, saturation, hue, impl=impl)\n",
    "  else:\n",
    "    return color_jitter_nonrand(\n",
    "        image, brightness, contrast, saturation, hue, impl=impl)\n",
    "\n",
    "\n",
    "def color_jitter_nonrand(image,\n",
    "                         brightness=0,\n",
    "                         contrast=0,\n",
    "                         saturation=0,\n",
    "                         hue=0,\n",
    "                         impl='simclrv2'):\n",
    "  \"\"\"Distorts the color of the image (jittering order is fixed).\n",
    "  Args:\n",
    "    image: The input image tensor.\n",
    "    brightness: A float, specifying the brightness for color jitter.\n",
    "    contrast: A float, specifying the contrast for color jitter.\n",
    "    saturation: A float, specifying the saturation for color jitter.\n",
    "    hue: A float, specifying the hue for color jitter.\n",
    "    impl: 'simclrv1' or 'simclrv2'.  Whether to use simclrv1 or simclrv2's\n",
    "        version of random brightness.\n",
    "  Returns:\n",
    "    The distorted image tensor.\n",
    "  \"\"\"\n",
    "  with tf.name_scope('distort_color'):\n",
    "    def apply_transform(i, x, brightness, contrast, saturation, hue):\n",
    "      \"\"\"Apply the i-th transformation.\"\"\"\n",
    "      if brightness != 0 and i == 0:\n",
    "        x = random_brightness(x, max_delta=brightness, impl=impl)\n",
    "      elif contrast != 0 and i == 1:\n",
    "        x = tf.image.random_contrast(\n",
    "            x, lower=1-contrast, upper=1+contrast)\n",
    "      elif saturation != 0 and i == 2:\n",
    "        x = tf.image.random_saturation(\n",
    "            x, lower=1-saturation, upper=1+saturation)\n",
    "      elif hue != 0:\n",
    "        x = tf.image.random_hue(x, max_delta=hue)\n",
    "      return x\n",
    "\n",
    "    for i in range(4):\n",
    "      image = apply_transform(i, image, brightness, contrast, saturation, hue)\n",
    "      image = tf.clip_by_value(image, 0., 1.)\n",
    "    return image\n",
    "\n",
    "\n",
    "def color_jitter_rand(image,\n",
    "                      brightness=0,\n",
    "                      contrast=0,\n",
    "                      saturation=0,\n",
    "                      hue=0,\n",
    "                      impl='simclrv2'):\n",
    "  \"\"\"Distorts the color of the image (jittering order is random).\n",
    "  Args:\n",
    "    image: The input image tensor.\n",
    "    brightness: A float, specifying the brightness for color jitter.\n",
    "    contrast: A float, specifying the contrast for color jitter.\n",
    "    saturation: A float, specifying the saturation for color jitter.\n",
    "    hue: A float, specifying the hue for color jitter.\n",
    "    impl: 'simclrv1' or 'simclrv2'.  Whether to use simclrv1 or simclrv2's\n",
    "        version of random brightness.\n",
    "  Returns:\n",
    "    The distorted image tensor.\n",
    "  \"\"\"\n",
    "  with tf.name_scope('distort_color'):\n",
    "    def apply_transform(i, x):\n",
    "      \"\"\"Apply the i-th transformation.\"\"\"\n",
    "      def brightness_foo():\n",
    "        if brightness == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return random_brightness(x, max_delta=brightness, impl=impl)\n",
    "\n",
    "      def contrast_foo():\n",
    "        if contrast == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return tf.image.random_contrast(x, lower=1-contrast, upper=1+contrast)\n",
    "      def saturation_foo():\n",
    "        if saturation == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return tf.image.random_saturation(\n",
    "              x, lower=1-saturation, upper=1+saturation)\n",
    "      def hue_foo():\n",
    "        if hue == 0:\n",
    "          return x\n",
    "        else:\n",
    "          return tf.image.random_hue(x, max_delta=hue)\n",
    "      x = tf.cond(tf.less(i, 2),\n",
    "                  lambda: tf.cond(tf.less(i, 1), brightness_foo, contrast_foo),\n",
    "                  lambda: tf.cond(tf.less(i, 3), saturation_foo, hue_foo))\n",
    "      return x\n",
    "\n",
    "    perm = tf.random.shuffle(tf.range(4))\n",
    "    for i in range(4):\n",
    "      image = apply_transform(perm[i], image)\n",
    "      image = tf.clip_by_value(image, 0., 1.)\n",
    "    return image\n",
    "\n",
    "\n",
    "def _compute_crop_shape(\n",
    "    image_height, image_width, aspect_ratio, crop_proportion):\n",
    "  \"\"\"Compute aspect ratio-preserving shape for central crop.\n",
    "  The resulting shape retains `crop_proportion` along one side and a proportion\n",
    "  less than or equal to `crop_proportion` along the other side.\n",
    "  Args:\n",
    "    image_height: Height of image to be cropped.\n",
    "    image_width: Width of image to be cropped.\n",
    "    aspect_ratio: Desired aspect ratio (width / height) of output.\n",
    "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
    "  Returns:\n",
    "    crop_height: Height of image after cropping.\n",
    "    crop_width: Width of image after cropping.\n",
    "  \"\"\"\n",
    "  image_width_float = tf.cast(image_width, tf.float32)\n",
    "  image_height_float = tf.cast(image_height, tf.float32)\n",
    "\n",
    "  def _requested_aspect_ratio_wider_than_image():\n",
    "    crop_height = tf.cast(\n",
    "        tf.math.rint(crop_proportion / aspect_ratio * image_width_float),\n",
    "        tf.int32)\n",
    "    crop_width = tf.cast(\n",
    "        tf.math.rint(crop_proportion * image_width_float), tf.int32)\n",
    "    return crop_height, crop_width\n",
    "\n",
    "  def _image_wider_than_requested_aspect_ratio():\n",
    "    crop_height = tf.cast(\n",
    "        tf.math.rint(crop_proportion * image_height_float), tf.int32)\n",
    "    crop_width = tf.cast(\n",
    "        tf.math.rint(crop_proportion * aspect_ratio * image_height_float),\n",
    "        tf.int32)\n",
    "    return crop_height, crop_width\n",
    "\n",
    "  return tf.cond(\n",
    "      aspect_ratio > image_width_float / image_height_float,\n",
    "      _requested_aspect_ratio_wider_than_image,\n",
    "      _image_wider_than_requested_aspect_ratio)\n",
    "\n",
    "\n",
    "def center_crop(image, height, width, crop_proportion):\n",
    "  \"\"\"Crops to center of image and rescales to desired size.\n",
    "  Args:\n",
    "    image: Image Tensor to crop.\n",
    "    height: Height of image to be cropped.\n",
    "    width: Width of image to be cropped.\n",
    "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
    "  Returns:\n",
    "    A `height` x `width` x channels Tensor holding a central crop of `image`.\n",
    "  \"\"\"\n",
    "  shape = tf.shape(image)\n",
    "  image_height = shape[0]\n",
    "  image_width = shape[1]\n",
    "  crop_height, crop_width = _compute_crop_shape(\n",
    "      image_height, image_width, width / height, crop_proportion)\n",
    "  offset_height = ((image_height - crop_height) + 1) // 2\n",
    "  offset_width = ((image_width - crop_width) + 1) // 2\n",
    "  image = tf.image.crop_to_bounding_box(\n",
    "      image, offset_height, offset_width, crop_height, crop_width)\n",
    "\n",
    "  image = tf.image.resize([image], [height, width],\n",
    "                          method=tf.image.ResizeMethod.BICUBIC)[0]\n",
    "\n",
    "  return image\n",
    "\n",
    "\n",
    "def distorted_bounding_box_crop(image,\n",
    "                                bbox,\n",
    "                                min_object_covered=0.1,\n",
    "                                aspect_ratio_range=(0.75, 1.33),\n",
    "                                area_range=(0.05, 1.0),\n",
    "                                max_attempts=100,\n",
    "                                scope=None):\n",
    "  \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n",
    "  See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
    "  Args:\n",
    "    image: `Tensor` of image data.\n",
    "    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n",
    "        where each coordinate is [0, 1) and the coordinates are arranged\n",
    "        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n",
    "        image.\n",
    "    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
    "        area of the image must contain at least this fraction of any bounding\n",
    "        box supplied.\n",
    "    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n",
    "        image must have an aspect ratio = width / height within this range.\n",
    "    area_range: An optional list of `float`s. The cropped area of the image\n",
    "        must contain a fraction of the supplied image within in this range.\n",
    "    max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
    "        region of the image of the specified constraints. After `max_attempts`\n",
    "        failures, return the entire image.\n",
    "    scope: Optional `str` for name scope.\n",
    "  Returns:\n",
    "    (cropped image `Tensor`, distorted bbox `Tensor`).\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope or 'distorted_bounding_box_crop'):\n",
    "    shape = tf.shape(image)\n",
    "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
    "        shape,\n",
    "        bounding_boxes=bbox,\n",
    "        min_object_covered=min_object_covered,\n",
    "        aspect_ratio_range=aspect_ratio_range,\n",
    "        area_range=area_range,\n",
    "        max_attempts=max_attempts,\n",
    "        use_image_if_no_bounding_boxes=True)\n",
    "    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n",
    "\n",
    "    # Crop the image to the specified bounding box.\n",
    "    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n",
    "    target_height, target_width, _ = tf.unstack(bbox_size)\n",
    "    image = tf.image.crop_to_bounding_box(\n",
    "        image, offset_y, offset_x, target_height, target_width)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def crop_and_resize(image, height, width):\n",
    "  \"\"\"Make a random crop and resize it to height `height` and width `width`.\n",
    "  Args:\n",
    "    image: Tensor representing the image.\n",
    "    height: Desired image height.\n",
    "    width: Desired image width.\n",
    "  Returns:\n",
    "    A `height` x `width` x channels Tensor holding a random crop of `image`.\n",
    "  \"\"\"\n",
    "  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n",
    "  aspect_ratio = width / height\n",
    "  image = distorted_bounding_box_crop(\n",
    "      image,\n",
    "      bbox,\n",
    "      min_object_covered=0.1,\n",
    "      aspect_ratio_range=(3. / 4 * aspect_ratio, 4. / 3. * aspect_ratio),\n",
    "      area_range=(0.08, 1.0),\n",
    "      max_attempts=100,\n",
    "      scope=None)\n",
    "  return tf.image.resize([image], [height, width],\n",
    "                         method=tf.image.ResizeMethod.BICUBIC)[0]\n",
    "\n",
    "\n",
    "def gaussian_blur(image, kernel_size, sigma, padding='SAME'):\n",
    "  \"\"\"Blurs the given image with separable convolution.\n",
    "  Args:\n",
    "    image: Tensor of shape [height, width, channels] and dtype float to blur.\n",
    "    kernel_size: Integer Tensor for the size of the blur kernel. This is should\n",
    "      be an odd number. If it is an even number, the actual kernel size will be\n",
    "      size + 1.\n",
    "    sigma: Sigma value for gaussian operator.\n",
    "    padding: Padding to use for the convolution. Typically 'SAME' or 'VALID'.\n",
    "  Returns:\n",
    "    A Tensor representing the blurred image.\n",
    "  \"\"\"\n",
    "  radius = tf.cast(kernel_size / 2, dtype=tf.int32)\n",
    "  kernel_size = radius * 2 + 1\n",
    "  x = tf.cast(tf.range(-radius, radius + 1), dtype=tf.float32)\n",
    "  blur_filter = tf.exp(-tf.pow(x, 2.0) /\n",
    "                       (2.0 * tf.pow(tf.cast(sigma, dtype=tf.float32), 2.0)))\n",
    "  blur_filter /= tf.reduce_sum(blur_filter)\n",
    "  # One vertical and one horizontal filter.\n",
    "  blur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n",
    "  blur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n",
    "  num_channels = tf.shape(image)[-1]\n",
    "  blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
    "  blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
    "  expand_batch_dim = image.shape.ndims == 3\n",
    "  if expand_batch_dim:\n",
    "    # Tensorflow requires batched input to convolutions, which we can fake with\n",
    "    # an extra dimension.\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "  blurred = tf.nn.depthwise_conv2d(\n",
    "      image, blur_h, strides=[1, 1, 1, 1], padding=padding)\n",
    "  blurred = tf.nn.depthwise_conv2d(\n",
    "      blurred, blur_v, strides=[1, 1, 1, 1], padding=padding)\n",
    "  if expand_batch_dim:\n",
    "    blurred = tf.squeeze(blurred, axis=0)\n",
    "  return blurred\n",
    "\n",
    "\n",
    "def random_crop_with_resize(image, height, width, p=1.0):\n",
    "  \"\"\"Randomly crop and resize an image.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    p: Probability of applying this transformation.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  def _transform(image):  # pylint: disable=missing-docstring\n",
    "    image = crop_and_resize(image, height, width)\n",
    "    return image\n",
    "  return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def random_color_jitter(image, p=1.0, strength=1.0,\n",
    "                        impl='simclrv2'):\n",
    "\n",
    "  def _transform(image):\n",
    "    color_jitter_t = functools.partial(\n",
    "        color_jitter, strength=strength, impl=impl)\n",
    "    image = random_apply(color_jitter_t, p=0.8, x=image)\n",
    "    return random_apply(to_grayscale, p=0.2, x=image)\n",
    "  return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def random_blur(image, height, width, p=1.0):\n",
    "  \"\"\"Randomly blur an image.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    p: probability of applying this transformation.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  del width\n",
    "  def _transform(image):\n",
    "    sigma = tf.random.uniform([], 0.1, 2.0, dtype=tf.float32)\n",
    "    return gaussian_blur(\n",
    "        image, kernel_size=height//10, sigma=sigma, padding='SAME')\n",
    "  return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def batch_random_blur(images_list, height, width, blur_probability=0.5):\n",
    "  \"\"\"Apply efficient batch data transformations.\n",
    "  Args:\n",
    "    images_list: a list of image tensors.\n",
    "    height: the height of image.\n",
    "    width: the width of image.\n",
    "    blur_probability: the probaility to apply the blur operator.\n",
    "  Returns:\n",
    "    Preprocessed feature list.\n",
    "  \"\"\"\n",
    "  def generate_selector(p, bsz):\n",
    "    shape = [bsz, 1, 1, 1]\n",
    "    selector = tf.cast(\n",
    "        tf.less(tf.random.uniform(shape, 0, 1, dtype=tf.float32), p),\n",
    "        tf.float32)\n",
    "    return selector\n",
    "\n",
    "  new_images_list = []\n",
    "  for images in images_list:\n",
    "    images_new = random_blur(images, height, width, p=1.)\n",
    "    selector = generate_selector(blur_probability, tf.shape(images)[0])\n",
    "    images = images_new * selector + images * (1 - selector)\n",
    "    images = tf.clip_by_value(images, 0., 1.)\n",
    "    new_images_list.append(images)\n",
    "\n",
    "  return new_images_list\n",
    "\n",
    "\n",
    "def preprocess_for_train(image,\n",
    "                         height,\n",
    "                         width,\n",
    "                         color_distort=True,\n",
    "                         crop=True,\n",
    "                         flip=True,\n",
    "                         impl='simclrv2'):\n",
    "  \"\"\"Preprocesses the given image for training.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    color_distort: Whether to apply the color distortion.\n",
    "    crop: Whether to crop the image.\n",
    "    flip: Whether or not to flip left and right of an image.\n",
    "    impl: 'simclrv1' or 'simclrv2'.  Whether to use simclrv1 or simclrv2's\n",
    "        version of random brightness.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  if crop:\n",
    "    image = random_crop_with_resize(image, height, width)\n",
    "  if flip:\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "  if color_distort:\n",
    "    image = random_color_jitter(image, strength=FLAGS.color_jitter_strength,\n",
    "                                impl=impl)\n",
    "  image = tf.reshape(image, [height, width, 3])\n",
    "  image = tf.clip_by_value(image, 0., 1.)\n",
    "  return image\n",
    "\n",
    "\n",
    "def preprocess_for_eval(image, height, width, crop=True):\n",
    "  \"\"\"Preprocesses the given image for evaluation.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    crop: Whether or not to (center) crop the test images.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "  \"\"\"\n",
    "  if crop:\n",
    "    image = center_crop(image, height, width, crop_proportion=CROP_PROPORTION)\n",
    "  image = tf.reshape(image, [height, width, 3])\n",
    "  image = tf.clip_by_value(image, 0., 1.)\n",
    "  return image\n",
    "\n",
    "\n",
    "def preprocess_image(image, height, width, is_training=False,\n",
    "                     color_distort=True, test_crop=True):\n",
    "  \"\"\"Preprocesses the given image.\n",
    "  Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    is_training: `bool` for whether the preprocessing is for training.\n",
    "    color_distort: whether to apply the color distortion.\n",
    "    test_crop: whether or not to extract a central crop of the images\n",
    "        (as for standard ImageNet evaluation) during the evaluation.\n",
    "  Returns:\n",
    "    A preprocessed image `Tensor` of range [0, 1].\n",
    "  \"\"\"\n",
    "  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "  if is_training:\n",
    "    return preprocess_for_train(image, height, width, color_distort)\n",
    "  else:\n",
    "    return preprocess_for_eval(image, height, width, test_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb07db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The SimCLR Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific simclr governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Functions and classes related to optimization (weight updates).\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "\n",
    "EETA_DEFAULT = 0.001\n",
    "\n",
    "\n",
    "class LARSOptimizer(tf.keras.optimizers.Optimizer):\n",
    "  \"\"\"Layer-wise Adaptive Rate Scaling for large batch training.\n",
    "  Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n",
    "  I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate,\n",
    "               momentum=0.9,\n",
    "               use_nesterov=False,\n",
    "               weight_decay=0.0,\n",
    "               exclude_from_weight_decay=None,\n",
    "               exclude_from_layer_adaptation=None,\n",
    "               classic_momentum=True,\n",
    "               eeta=EETA_DEFAULT,\n",
    "               name=\"LARSOptimizer\"):\n",
    "    \"\"\"Constructs a LARSOptimizer.\n",
    "    Args:\n",
    "      learning_rate: A `float` for learning rate.\n",
    "      momentum: A `float` for momentum.\n",
    "      use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n",
    "      weight_decay: A `float` for weight decay.\n",
    "      exclude_from_weight_decay: A list of `string` for variable screening, if\n",
    "          any of the string appears in a variable's name, the variable will be\n",
    "          excluded for computing weight decay. For example, one could specify\n",
    "          the list like ['batch_normalization', 'bias'] to exclude BN and bias\n",
    "          from weight decay.\n",
    "      exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n",
    "          for layer adaptation. If it is None, it will be defaulted the same as\n",
    "          exclude_from_weight_decay.\n",
    "      classic_momentum: A `boolean` for whether to use classic (or popular)\n",
    "          momentum. The learning rate is applied during momeuntum update in\n",
    "          classic momentum, but after momentum for popular momentum.\n",
    "      eeta: A `float` for scaling of learning rate when computing trust ratio.\n",
    "      name: The name for the scope.\n",
    "    \"\"\"\n",
    "    super(LARSOptimizer, self).__init__(name)\n",
    "\n",
    "    self._set_hyper(\"learning_rate\", learning_rate)\n",
    "    self.momentum = momentum\n",
    "    self.weight_decay = weight_decay\n",
    "    self.use_nesterov = use_nesterov\n",
    "    self.classic_momentum = classic_momentum\n",
    "    self.eeta = eeta\n",
    "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "    # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n",
    "    # arg is None.\n",
    "    if exclude_from_layer_adaptation:\n",
    "      self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
    "    else:\n",
    "      self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
    "\n",
    "  def _create_slots(self, var_list):\n",
    "    for v in var_list:\n",
    "      self.add_slot(v, \"Momentum\")\n",
    "\n",
    "  def _resource_apply_dense(self, grad, param, apply_state=None):\n",
    "    if grad is None or param is None:\n",
    "      return tf.no_op()\n",
    "\n",
    "    var_device, var_dtype = param.device, param.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype)) or\n",
    "                    self._fallback_apply_state(var_device, var_dtype))\n",
    "    learning_rate = coefficients[\"lr_t\"]\n",
    "\n",
    "    param_name = param.name\n",
    "\n",
    "    v = self.get_slot(param, \"Momentum\")\n",
    "\n",
    "    if self._use_weight_decay(param_name):\n",
    "      grad += self.weight_decay * param\n",
    "\n",
    "    if self.classic_momentum:\n",
    "      trust_ratio = 1.0\n",
    "      if self._do_layer_adaptation(param_name):\n",
    "        w_norm = tf.norm(param, ord=2)\n",
    "        g_norm = tf.norm(grad, ord=2)\n",
    "        trust_ratio = tf.where(\n",
    "            tf.greater(w_norm, 0),\n",
    "            tf.where(tf.greater(g_norm, 0), (self.eeta * w_norm / g_norm), 1.0),\n",
    "            1.0)\n",
    "      scaled_lr = learning_rate * trust_ratio\n",
    "\n",
    "      next_v = tf.multiply(self.momentum, v) + scaled_lr * grad\n",
    "      if self.use_nesterov:\n",
    "        update = tf.multiply(self.momentum, next_v) + scaled_lr * grad\n",
    "      else:\n",
    "        update = next_v\n",
    "      next_param = param - update\n",
    "    else:\n",
    "      next_v = tf.multiply(self.momentum, v) + grad\n",
    "      if self.use_nesterov:\n",
    "        update = tf.multiply(self.momentum, next_v) + grad\n",
    "      else:\n",
    "        update = next_v\n",
    "\n",
    "      trust_ratio = 1.0\n",
    "      if self._do_layer_adaptation(param_name):\n",
    "        w_norm = tf.norm(param, ord=2)\n",
    "        v_norm = tf.norm(update, ord=2)\n",
    "        trust_ratio = tf.where(\n",
    "            tf.greater(w_norm, 0),\n",
    "            tf.where(tf.greater(v_norm, 0), (self.eeta * w_norm / v_norm), 1.0),\n",
    "            1.0)\n",
    "      scaled_lr = trust_ratio * learning_rate\n",
    "      next_param = param - scaled_lr * update\n",
    "\n",
    "    return tf.group(*[\n",
    "        param.assign(next_param, use_locking=False),\n",
    "        v.assign(next_v, use_locking=False)\n",
    "    ])\n",
    "\n",
    "  def _use_weight_decay(self, param_name):\n",
    "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
    "    if not self.weight_decay:\n",
    "      return False\n",
    "    if self.exclude_from_weight_decay:\n",
    "      for r in self.exclude_from_weight_decay:\n",
    "        # TODO(srbs): Try to avoid name based filtering.\n",
    "        if re.search(r, param_name) is not None:\n",
    "          return False\n",
    "    return True\n",
    "\n",
    "  def _do_layer_adaptation(self, param_name):\n",
    "    \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n",
    "    if self.exclude_from_layer_adaptation:\n",
    "      for r in self.exclude_from_layer_adaptation:\n",
    "        # TODO(srbs): Try to avoid name based filtering.\n",
    "        if re.search(r, param_name) is not None:\n",
    "          return False\n",
    "    return True\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(LARSOptimizer, self).get_config()\n",
    "    config.update({\n",
    "        \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "        \"momentum\": self.momentum,\n",
    "        \"classic_momentum\": self.classic_momentum,\n",
    "        \"weight_decay\": self.weight_decay,\n",
    "        \"eeta\": self.eeta,\n",
    "        \"use_nesterov\": self.use_nesterov,\n",
    "    })\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b396a3",
   "metadata": {},
   "source": [
    "## Import des données de Stanford Dogs et Preprocess\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3746114d",
   "metadata": {},
   "source": [
    "Je récupérer les données nécessaire pour entrainer les modèles. Je récupére 20% des données du Dataset de Test de Standford Dogs pour cela. Les données du train ont été utiliser pour le pre entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80348517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load tensorflow datasets: we use tensorflow flower dataset as an example\n",
    "\n",
    "batch_size = 64\n",
    "dataset_name = 'stanford_dogs'\n",
    "\n",
    "tfds_dataset, tfds_info = tfds.load(\n",
    "    dataset_name, split='test[:20%]',  with_info=True)\n",
    "\n",
    "builder = tfds.builder('stanford_dogs')\n",
    "tfds_dataset = builder.as_dataset(split='test[:20%]', decoders=tfds.decode.PartialDecoding({\n",
    "    'image': True,\n",
    "    'label' : True\n",
    "}))\n",
    "\n",
    "num_images = tfds_info.splits['train'].num_examples\n",
    "num_classes = tfds_info.features['label'].num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a5dec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(x):\n",
    "    x['image'] = preprocess_image(\n",
    "          x['image'], 224, 224, is_training=False, color_distort=False)\n",
    "    return x\n",
    "\n",
    "ds = tfds_dataset.map(_preprocess).batch(batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6209f180",
   "metadata": {},
   "source": [
    "Récuperation des données de Test. Nous utiliserons le reste des données de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e35063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_test = tfds.builder('stanford_dogs')\n",
    "tfds_dataset_test = builder.as_dataset(split='test[20%:]', decoders=tfds.decode.PartialDecoding({\n",
    "    'image': True,\n",
    "    'label' : True\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "474ebd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = tfds_dataset_test.map(_preprocess).batch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24122150",
   "metadata": {},
   "source": [
    "## SimCLR pre entrainer sur Stanford Dogs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04866c08",
   "metadata": {},
   "source": [
    "Le pre entrainement et le fine tuning ont été réaliseé grâce au routine du répertoire git. Ici nous récuperons juste le modèle et nous les testons sur les données de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eee5edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(model_path, log=False):\n",
    "  if log:\n",
    "    print(\"Loading model from %s\" % model_path)\n",
    "  model = tf.saved_model.load(model_path)\n",
    "  if log:\n",
    "    print(\"Loaded model!\")\n",
    "  top_1_accuracy = tf.keras.metrics.Accuracy('top_1_accuracy')\n",
    "  for i, features in enumerate(ds_test):\n",
    "    logits = model(features[\"image\"], trainable=False)[\"logits_sup\"]\n",
    "    top_1_accuracy.update_state(features[\"label\"], tf.argmax(logits, axis=-1))\n",
    "    if log and (i + 1) % 50 == 0:\n",
    "      print(\"Finished %d examples\" % ((i + 1) * BATCH_SIZE))\n",
    "  return top_1_accuracy.result().numpy().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c08fd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31240) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35150) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32610) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33800) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32900) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34270) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33340) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31770) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32890) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32510) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32760) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34820) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31940) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32870) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35340) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32780) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32470) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31260) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34280) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32000) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35920) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31390) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35660) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32190) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33290) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33530) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_36050) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33120) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33690) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31440) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32690) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31620) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34590) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31710) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31650) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31970) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35610) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34500) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33050) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32620) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34880) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34800) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31460) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34200) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35440) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33700) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35000) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34300) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33990) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34580) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33670) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_36030) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35430) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34370) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31100) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34000) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32260) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33140) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35780) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34780) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35690) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34740) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_36070) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32330) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32840) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35750) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33250) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34410) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33720) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33830) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32580) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31340) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35730) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34510) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35080) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35740) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34990) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33470) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35180) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34860) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34610) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34070) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32550) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34660) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32920) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34050) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35060) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33780) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34480) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35030) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34140) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35940) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33020) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35300) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34600) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32660) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34010) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31910) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32040) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33030) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32720) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31220) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32570) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35090) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31320) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35550) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31580) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32630) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31090) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35840) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_36090) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34460) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34970) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31070) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32990) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34160) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33790) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32710) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34320) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34250) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34290) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31540) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32850) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32280) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33570) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32880) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33450) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32160) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34040) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33300) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34560) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35170) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35160) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35280) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31640) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34620) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_36000) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32590) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31780) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32320) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33880) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33760) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32250) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32390) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31490) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35590) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34570) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31830) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35800) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33130) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34230) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_36060) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34870) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34360) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31720) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35070) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31420) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31810) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31360) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33900) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33870) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35320) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35010) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_36020) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34550) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35570) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32820) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32310) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35390) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35770) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32440) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32960) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33380) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31480) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34690) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33260) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33200) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34130) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32740) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33010) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35930) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35990) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34770) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34980) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33730) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33600) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34790) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32120) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31550) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34930) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32520) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34850) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35580) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34240) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34110) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33170) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33150) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31180) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31270) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31080) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32210) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34910) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35370) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35110) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33660) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32910) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33460) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34530) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35470) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_36100) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32450) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35360) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34120) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34210) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34710) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31820) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31230) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35380) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32230) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32170) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33710) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34830) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34960) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32600) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32220) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32110) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31280) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33480) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33080) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31470) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34760) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33920) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33420) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31120) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32010) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31840) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31880) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34430) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34680) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31140) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32360) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32080) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34100) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35980) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33620) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35460) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31200) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34080) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33680) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35710) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31870) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33070) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33400) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35760) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31400) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31170) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34520) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33310) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34150) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34730) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35540) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32650) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31900) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31410) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35620) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33280) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35600) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32670) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34030) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34310) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32350) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31790) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31350) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34470) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34950) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31500) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35290) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32500) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32340) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33500) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32940) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35120) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33630) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34090) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32930) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31930) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33940) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33270) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35210) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32680) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33970) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34060) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31330) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35860) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33190) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31990) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34380) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33490) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31250) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35490) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35450) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31920) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32370) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33890) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34720) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31960) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33000) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32530) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33750) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33180) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34750) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32540) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35970) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33770) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34260) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32750) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34900) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33240) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32030) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34630) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33220) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31860) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34340) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32790) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35230) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32020) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35420) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33100) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32140) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35190) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33390) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31690) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32830) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34450) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33060) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33320) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31890) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34390) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33580) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35630) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32090) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33860) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34400) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34350) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34170) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34640) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31380) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33560) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35900) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35020) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32640) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32300) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35260) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33350) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34330) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34670) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32770) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35790) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32290) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31630) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35670) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32270) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33160) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35240) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_36080) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35040) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31530) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32240) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35520) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31450) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33610) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31850) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35870) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34180) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34920) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31730) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35680) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35410) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31290) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33910) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33540) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_36040) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35530) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32180) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31660) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31160) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35810) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35880) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32060) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35350) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35270) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35250) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32150) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33960) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32430) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34540) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33230) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31520) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33650) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35050) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33740) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34940) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32560) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35500) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35480) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35820) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35650) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31560) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32070) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34490) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31610) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33520) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34650) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33330) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34810) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33590) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31760) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35910) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33410) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31370) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33810) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35330) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32460) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35100) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35510) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32200) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35220) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34440) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32860) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34220) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32400) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33550) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33110) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_36010) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35950) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33950) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35560) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35830) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32420) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32970) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31600) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35960) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32410) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34840) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35850) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35400) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33840) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32730) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31680) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33440) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31310) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33510) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31510) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34700) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32480) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31430) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35720) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31670) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33360) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31130) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31980) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34190) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32700) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31300) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31210) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35310) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32950) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33430) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31700) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33040) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32810) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33930) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33210) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32490) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35640) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35200) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32130) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31570) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31110) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32800) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31750) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31190) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35130) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35140) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33980) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31590) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33640) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34420) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32380) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31150) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35700) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32100) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33820) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34020) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31800) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31740) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32050) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33370) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_35890) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_31950) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_32980) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33850) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_33090) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_34890) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "path = \"simCLR/simCLR_pretrain_ter_finetune/saved_model/10807/\"\n",
    "results_test = {}\n",
    "results_test[path] = eval_test(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c6463ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simCLR/simCLR_pretrain_ter_finetune/saved_model/10807/\n",
      "Top-1: 6.8\n"
     ]
    }
   ],
   "source": [
    "print(path)\n",
    "print(\"Top-1: %.1f\" % (results_test[path] * 100))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4142e53",
   "metadata": {},
   "source": [
    "Voici les performances du modèle de SimCLR pre entrainer sur Stanford Dogs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c6433f",
   "metadata": {},
   "source": [
    "## SimCLR pre entrainer sur ImageNet"
   ]
  },
  {
   "cell_type": "raw",
   "id": "524ec75a",
   "metadata": {},
   "source": [
    "Dans cette partie, nous utiliserons le modèle de SimCLR entrainer par les auteurs sur ImageNet et nous allons le fine tune sur les données de Stanfords Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a4168a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference___call___15879) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "#@title Load module and construct the computation graph\n",
    "\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 0.\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, path):\n",
    "    super(Model, self).__init__()\n",
    "    self.saved_model = tf.saved_model.load(path)\n",
    "    self.dense_layer = tf.keras.layers.Dense(units=num_classes, name=\"head_supervised_new\")\n",
    "    self.optimizer = LARSOptimizer(\n",
    "      learning_rate,\n",
    "      momentum=momentum,\n",
    "      weight_decay=weight_decay,\n",
    "      exclude_from_weight_decay=['batch_normalization', 'bias', 'head_supervised'])\n",
    "\n",
    "  def call(self, x):\n",
    "    with tf.GradientTape() as tape:\n",
    "      outputs = self.saved_model(x['image'], trainable=False)\n",
    "      print(outputs)\n",
    "      logits_t = self.dense_layer(outputs['final_avg_pool'])\n",
    "      loss_t = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels = tf.one_hot(x['label'], num_classes), logits=logits_t))\n",
    "      dense_layer_weights = self.dense_layer.trainable_weights\n",
    "      print('Variables to train:', dense_layer_weights)\n",
    "      grads = tape.gradient(loss_t, dense_layer_weights)\n",
    "      self.optimizer.apply_gradients(zip(grads, dense_layer_weights))\n",
    "    return loss_t, x[\"image\"], logits_t, x[\"label\"]\n",
    "\n",
    "  def infer(self,x):\n",
    "      outputs = self.saved_model(x['image'], trainable=False)\n",
    "      logits_t = self.dense_layer(outputs['final_avg_pool'])\n",
    "      loss_t = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = tf.one_hot(x['label'], num_classes), logits=logits_t))\n",
    "      return loss_t, x[\"image\"], logits_t, x[\"label\"]\n",
    "        \n",
    "\n",
    "model = Model(\"simCLR/simCLR_4x_pretrain/saved_model\")\n",
    "\n",
    "# Remove this for debugging.  \n",
    "@tf.function\n",
    "def train_step(x):\n",
    "  return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "811df917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(64, 56, 56, 1024) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(64, 28, 28, 2048) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(64, 7, 7, 8192) dtype=float32>, 'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(64, 56, 56, 256) dtype=float32>, 'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(64, 14, 14, 4096) dtype=float32>, 'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(64, 8192) dtype=float32>, 'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(64, 112, 112, 256) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(64, 1000) dtype=float32>}\n",
      "Variables to train: [<tf.Variable 'model/head_supervised_new/kernel:0' shape=(8192, 120) dtype=float32>, <tf.Variable 'model/head_supervised_new/bias:0' shape=(120,) dtype=float32>]\n",
      "{'block_group1': <tf.Tensor 'model/StatefulPartitionedCall:0' shape=(64, 56, 56, 1024) dtype=float32>, 'block_group2': <tf.Tensor 'model/StatefulPartitionedCall:1' shape=(64, 28, 28, 2048) dtype=float32>, 'block_group4': <tf.Tensor 'model/StatefulPartitionedCall:3' shape=(64, 7, 7, 8192) dtype=float32>, 'initial_max_pool': <tf.Tensor 'model/StatefulPartitionedCall:6' shape=(64, 56, 56, 256) dtype=float32>, 'block_group3': <tf.Tensor 'model/StatefulPartitionedCall:2' shape=(64, 14, 14, 4096) dtype=float32>, 'final_avg_pool': <tf.Tensor 'model/StatefulPartitionedCall:4' shape=(64, 8192) dtype=float32>, 'initial_conv': <tf.Tensor 'model/StatefulPartitionedCall:5' shape=(64, 112, 112, 256) dtype=float32>, 'logits_sup': <tf.Tensor 'model/StatefulPartitionedCall:7' shape=(64, 1000) dtype=float32>}\n",
      "Variables to train: [<tf.Variable 'model/head_supervised_new/kernel:0' shape=(8192, 120) dtype=float32>, <tf.Variable 'model/head_supervised_new/bias:0' shape=(120,) dtype=float32>]\n",
      "[Iter 1] Loss: 4.816432476043701 Top 1: 0.03125\n",
      "[Iter 2] Loss: 4.785896301269531 Top 1: 0.0\n",
      "[Iter 3] Loss: 4.781671524047852 Top 1: 0.015625\n",
      "[Iter 4] Loss: 4.719189643859863 Top 1: 0.046875\n",
      "[Iter 5] Loss: 4.682836532592773 Top 1: 0.046875\n",
      "[Iter 6] Loss: 4.573892116546631 Top 1: 0.21875\n",
      "[Iter 7] Loss: 4.544224739074707 Top 1: 0.171875\n",
      "[Iter 8] Loss: 4.489171028137207 Top 1: 0.140625\n",
      "[Iter 9] Loss: 4.361902236938477 Top 1: 0.171875\n",
      "[Iter 10] Loss: 4.318881988525391 Top 1: 0.125\n",
      "[Iter 11] Loss: 4.346336841583252 Top 1: 0.109375\n",
      "[Iter 12] Loss: 4.32493782043457 Top 1: 0.109375\n",
      "[Iter 13] Loss: 4.13832950592041 Top 1: 0.234375\n",
      "[Iter 14] Loss: 3.8851475715637207 Top 1: 0.328125\n",
      "[Iter 15] Loss: 4.016016006469727 Top 1: 0.25\n",
      "[Iter 16] Loss: 3.9111649990081787 Top 1: 0.171875\n",
      "[Iter 17] Loss: 3.7457165718078613 Top 1: 0.3125\n",
      "[Iter 18] Loss: 3.727151870727539 Top 1: 0.265625\n",
      "[Iter 19] Loss: 3.4978103637695312 Top 1: 0.34375\n",
      "[Iter 20] Loss: 3.6536409854888916 Top 1: 0.25\n",
      "[Iter 21] Loss: 3.5326473712921143 Top 1: 0.328125\n",
      "[Iter 22] Loss: 3.3618717193603516 Top 1: 0.359375\n",
      "[Iter 23] Loss: 3.3232290744781494 Top 1: 0.375\n",
      "[Iter 24] Loss: 3.33408784866333 Top 1: 0.375\n",
      "[Iter 25] Loss: 3.431889533996582 Top 1: 0.296875\n",
      "[Iter 26] Loss: 3.2319955825805664 Top 1: 0.359375\n"
     ]
    }
   ],
   "source": [
    "#@title We fine-tune the new *linear layer* for just a few iterations.\n",
    "\n",
    "total_iterations = 26\n",
    "iterator = iter(ds)\n",
    "for it in range(total_iterations):\n",
    "  x = next(iterator)\n",
    "  loss, image, logits, labels = train_step(x)\n",
    "  logits = logits.numpy()\n",
    "  labels = labels.numpy()\n",
    "  pred = logits.argmax(-1)\n",
    "  correct = np.sum(pred == labels)\n",
    "  total = labels.size\n",
    "  print(\"[Iter {}] Loss: {} Top 1: {}\".format(it+1, loss, correct/float(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfa2f7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1: 62.5\n"
     ]
    }
   ],
   "source": [
    "for i, features in enumerate(ds_test):\n",
    "    top_1_accuracy = tf.keras.metrics.Accuracy('top_1_accuracy')\n",
    "    \n",
    "    loss, image, logits, labels = model.infer(features)\n",
    "    \n",
    "    top_1_accuracy.update_state(features[\"label\"], tf.argmax(logits, axis=-1))\n",
    "    \n",
    "print(\"Top-1: %.1f\" % (top_1_accuracy.result().numpy().astype(float) * 100))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d560686",
   "metadata": {},
   "source": [
    "Ceci donne une idée des performances que nous aurions pu obtenir si nous avions pu mieux optimiser les paramètres de SimCLR pendant le pré entrainement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05850064",
   "metadata": {},
   "source": [
    "# Partie 2 : Inceptionv3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ffe7189",
   "metadata": {},
   "source": [
    "Dans cette partie, nous entrainons le modèle qui nous servira de baseline. Pour cela, je dois transformer les données dans un premier temps pour les mettre dans un array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b25eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac29d2d",
   "metadata": {},
   "source": [
    "## Preprocess des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5345cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_unbatch = ds.unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb802885",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_unbatch = ds_test.unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62265888",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(ds_unbatch.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc365ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = list(ds_test_unbatch.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "629738e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(0,len(dataset)):\n",
    "    X.append(dataset[i]['image'])\n",
    "    y.append(dataset[i]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "366562bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(0,len(dataset_test)):\n",
    "    X_test.append(dataset_test[i]['image'])\n",
    "    y_test.append(dataset_test[i]['label'])\n",
    "    \n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a84047b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "068c4b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Size of created sets :\n",
      "--------------------------------------------------\n",
      "Val set size =  687\n",
      "Test set size =  6177\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### Create train and test set\n",
    "x_test, x_val, y_test, y_val = train_test_split(X_test, y_test, test_size = 0.1, random_state = 42)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Size of created sets :\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Val set size = \",x_val.shape[0])\n",
    "print(\"Test set size = \",x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf5ae7",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25916078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = base_model.output\n",
    "x = Flatten()(x) # Flatten dimensions to for use in FC layers\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) # Dropout layer to reduce overfitting\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(num_classes, activation='softmax')(x) # Softmax for multiclass\n",
    "inception_v3 = Model(inputs=base_model.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ce52238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, horizontal_flip=True, fill_mode=\"nearest\", preprocessing_function = tf.keras.applications.inception_v3.preprocess_input)\n",
    "val_datagen = ImageDataGenerator(preprocessing_function = tf.keras.applications.inception_v3.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "618e06df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antoi\\miniconda3\\envs\\Test\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 21s 69ms/step - loss: 4.7961 - accuracy: 0.0122 - val_loss: 4.7946 - val_accuracy: 0.0073\n",
      "Epoch 2/26\n",
      "215/215 [==============================] - 14s 63ms/step - loss: 4.7728 - accuracy: 0.0128 - val_loss: 4.7973 - val_accuracy: 0.0063\n",
      "Epoch 3/26\n",
      "215/215 [==============================] - 13s 62ms/step - loss: 4.7595 - accuracy: 0.0128 - val_loss: 4.7886 - val_accuracy: 0.0115\n",
      "Epoch 4/26\n",
      "215/215 [==============================] - 13s 62ms/step - loss: 4.7506 - accuracy: 0.0192 - val_loss: 4.7937 - val_accuracy: 0.0083\n",
      "Epoch 5/26\n",
      "215/215 [==============================] - 13s 62ms/step - loss: 4.7375 - accuracy: 0.0186 - val_loss: 4.7981 - val_accuracy: 0.0094\n",
      "Epoch 6/26\n",
      "215/215 [==============================] - 13s 62ms/step - loss: 4.7294 - accuracy: 0.0146 - val_loss: 4.7939 - val_accuracy: 0.0135\n",
      "Epoch 7/26\n",
      "215/215 [==============================] - 13s 63ms/step - loss: 4.7264 - accuracy: 0.0169 - val_loss: 4.7843 - val_accuracy: 0.0083\n",
      "Epoch 8/26\n",
      "215/215 [==============================] - 14s 63ms/step - loss: 4.7156 - accuracy: 0.0256 - val_loss: 4.7928 - val_accuracy: 0.0073\n",
      "Epoch 9/26\n",
      "215/215 [==============================] - 14s 65ms/step - loss: 4.7051 - accuracy: 0.0245 - val_loss: 4.7824 - val_accuracy: 0.0094\n",
      "Epoch 10/26\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 4.7006 - accuracy: 0.0251 - val_loss: 4.7892 - val_accuracy: 0.0115\n",
      "Epoch 11/26\n",
      "215/215 [==============================] - 19s 88ms/step - loss: 4.6882 - accuracy: 0.0192 - val_loss: 4.7879 - val_accuracy: 0.0094\n",
      "Epoch 12/26\n",
      "215/215 [==============================] - 17s 77ms/step - loss: 4.6983 - accuracy: 0.0233 - val_loss: 4.7930 - val_accuracy: 0.0208\n",
      "Epoch 13/26\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 4.6847 - accuracy: 0.0262 - val_loss: 4.7926 - val_accuracy: 0.0167\n",
      "Epoch 14/26\n",
      "215/215 [==============================] - 13s 62ms/step - loss: 4.6617 - accuracy: 0.0303 - val_loss: 4.7948 - val_accuracy: 0.0146\n",
      "Epoch 15/26\n",
      "215/215 [==============================] - 13s 62ms/step - loss: 4.6705 - accuracy: 0.0297 - val_loss: 4.7819 - val_accuracy: 0.0104\n",
      "Epoch 16/26\n",
      "215/215 [==============================] - 13s 62ms/step - loss: 4.6394 - accuracy: 0.0338 - val_loss: 4.7791 - val_accuracy: 0.0146\n",
      "Epoch 17/26\n",
      "215/215 [==============================] - 13s 63ms/step - loss: 4.6482 - accuracy: 0.0297 - val_loss: 4.7889 - val_accuracy: 0.0219\n",
      "Epoch 18/26\n",
      "215/215 [==============================] - 13s 62ms/step - loss: 4.6477 - accuracy: 0.0216 - val_loss: 4.7871 - val_accuracy: 0.0104\n",
      "Epoch 19/26\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 4.6314 - accuracy: 0.0344 - val_loss: 4.7875 - val_accuracy: 0.0177\n",
      "Epoch 20/26\n",
      "215/215 [==============================] - 14s 66ms/step - loss: 4.6261 - accuracy: 0.0262 - val_loss: 4.7977 - val_accuracy: 0.0167\n",
      "Epoch 21/26\n",
      "215/215 [==============================] - 14s 66ms/step - loss: 4.6226 - accuracy: 0.0350 - val_loss: 4.7930 - val_accuracy: 0.0125\n",
      "Epoch 22/26\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 4.6157 - accuracy: 0.0297 - val_loss: 4.7883 - val_accuracy: 0.0156\n",
      "Epoch 23/26\n",
      "215/215 [==============================] - 14s 63ms/step - loss: 4.5987 - accuracy: 0.0402 - val_loss: 4.7995 - val_accuracy: 0.0188\n",
      "Epoch 24/26\n",
      "215/215 [==============================] - 14s 63ms/step - loss: 4.5892 - accuracy: 0.0367 - val_loss: 4.7916 - val_accuracy: 0.0135\n",
      "Epoch 25/26\n",
      "215/215 [==============================] - 14s 65ms/step - loss: 4.5816 - accuracy: 0.0361 - val_loss: 4.7928 - val_accuracy: 0.0208\n",
      "Epoch 26/26\n",
      "215/215 [==============================] - 15s 68ms/step - loss: 4.5714 - accuracy: 0.0402 - val_loss: 4.7871 - val_accuracy: 0.0198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12fc6ef6af0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "inception_v3.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "               optimizer=keras.optimizers.Adam(lr=learning_rate), metrics=[\"accuracy\"])\n",
    "\n",
    "inception_v3.fit(train_datagen.flow(X, y, batch_size=8), validation_data=val_datagen.flow(x_val, y_val, batch_size=8), epochs=26, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e0a71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec8fd955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inceptionv3(hp):\n",
    "    \n",
    "    inceptionv3_model = InceptionV3(weights='imagenet',include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    for layer in inceptionv3_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    x = inceptionv3_model.output\n",
    "    x = Flatten()(x) # Flatten dimensions to for use in FC layers\n",
    "    \n",
    "    hp_units = hp.Int('units', min_value=256, max_value=512, step=128)\n",
    "    x = Dense(units=hp_units, activation='relu')(x)\n",
    "    \n",
    "    rate=hp.Float('dropout_2', min_value=0.0, max_value=0.3, step=0.1)\n",
    "    x = Dropout(rate)(x) # Dropout layer to reduce overfitting\n",
    "    \n",
    "    hp_units_2 = hp.Int('units_2', min_value=128, max_value=256, step=128)\n",
    "    x = Dense(hp_units_2, activation='relu')(x)\n",
    "    \n",
    "    x = Dense(num_classes, activation='softmax')(x) # Softmax for multiclass\n",
    "    inceptionv3 = Model(inputs=inceptionv3_model.input, outputs=x)\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "\n",
    "    inceptionv3.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return inceptionv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ea92a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project .\\untitled_project\\oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project .\\untitled_project\\oracle.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from .\\untitled_project\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from .\\untitled_project\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner_inceptionv3 = kt.Hyperband(model_inceptionv3,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4cf16fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete. The optimal learning rate for the optimizer\n",
      "is 0.0001.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner_inceptionv3.search(train_datagen.flow(X, y, batch_size=8),\n",
    "            epochs=50, \n",
    "            validation_data=val_datagen.flow(x_val, y_val, batch_size=8))\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps_inceptionv3=tuner_inceptionv3.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal learning rate for the optimizer\n",
    "is {best_hps_inceptionv3.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e034890a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'units': 384,\n",
       " 'dropout_2': 0.1,\n",
       " 'units_2': 256,\n",
       " 'learning_rate': 0.0001,\n",
       " 'tuner/epochs': 10,\n",
       " 'tuner/initial_epoch': 4,\n",
       " 'tuner/bracket': 2,\n",
       " 'tuner/round': 2,\n",
       " 'tuner/trial_id': '7541870606d9d6e93e839059fb7ab513'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps_inceptionv3.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17068f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "215/215 [==============================] - 22s 88ms/step - loss: 4.8364 - accuracy: 0.0111 - val_loss: 4.8068 - val_accuracy: 0.0167\n",
      "Epoch 2/50\n",
      "215/215 [==============================] - 25s 114ms/step - loss: 4.7561 - accuracy: 0.0134 - val_loss: 4.8040 - val_accuracy: 0.0104\n",
      "Epoch 3/50\n",
      "215/215 [==============================] - 15s 71ms/step - loss: 4.7339 - accuracy: 0.0157 - val_loss: 4.7883 - val_accuracy: 0.0146\n",
      "Epoch 4/50\n",
      "215/215 [==============================] - 17s 79ms/step - loss: 4.7047 - accuracy: 0.0181 - val_loss: 4.8304 - val_accuracy: 0.0219\n",
      "Epoch 5/50\n",
      "215/215 [==============================] - 20s 92ms/step - loss: 4.6775 - accuracy: 0.0233 - val_loss: 4.8324 - val_accuracy: 0.0125\n",
      "Epoch 6/50\n",
      "215/215 [==============================] - 14s 67ms/step - loss: 4.6498 - accuracy: 0.0274 - val_loss: 4.7997 - val_accuracy: 0.0125\n",
      "Epoch 7/50\n",
      "215/215 [==============================] - 22s 102ms/step - loss: 4.6345 - accuracy: 0.0251 - val_loss: 4.7782 - val_accuracy: 0.0167\n",
      "Epoch 8/50\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 4.6088 - accuracy: 0.0297 - val_loss: 4.7885 - val_accuracy: 0.0198\n",
      "Epoch 9/50\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 4.5886 - accuracy: 0.0274 - val_loss: 4.8119 - val_accuracy: 0.0104\n",
      "Epoch 10/50\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 4.5890 - accuracy: 0.0321 - val_loss: 4.7824 - val_accuracy: 0.0167\n",
      "Epoch 11/50\n",
      "215/215 [==============================] - 16s 77ms/step - loss: 4.5794 - accuracy: 0.0344 - val_loss: 4.7922 - val_accuracy: 0.0146\n",
      "Epoch 12/50\n",
      "215/215 [==============================] - 28s 131ms/step - loss: 4.5631 - accuracy: 0.0303 - val_loss: 4.8047 - val_accuracy: 0.0146\n",
      "Epoch 13/50\n",
      "215/215 [==============================] - 26s 120ms/step - loss: 4.5475 - accuracy: 0.0385 - val_loss: 4.8542 - val_accuracy: 0.0146\n",
      "Epoch 14/50\n",
      "215/215 [==============================] - 30s 141ms/step - loss: 4.5455 - accuracy: 0.0367 - val_loss: 4.7770 - val_accuracy: 0.0219\n",
      "Epoch 15/50\n",
      "215/215 [==============================] - 36s 170ms/step - loss: 4.5234 - accuracy: 0.0338 - val_loss: 4.7732 - val_accuracy: 0.0125\n",
      "Epoch 16/50\n",
      "215/215 [==============================] - 28s 127ms/step - loss: 4.5256 - accuracy: 0.0355 - val_loss: 4.7544 - val_accuracy: 0.0177\n",
      "Epoch 17/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 4.5151 - accuracy: 0.0297 - val_loss: 4.7936 - val_accuracy: 0.0177\n",
      "Epoch 18/50\n",
      "215/215 [==============================] - 25s 116ms/step - loss: 4.5078 - accuracy: 0.0443 - val_loss: 4.7928 - val_accuracy: 0.0177\n",
      "Epoch 19/50\n",
      "215/215 [==============================] - 28s 129ms/step - loss: 4.4741 - accuracy: 0.0420 - val_loss: 4.7638 - val_accuracy: 0.0156\n",
      "Epoch 20/50\n",
      "215/215 [==============================] - 26s 120ms/step - loss: 4.4826 - accuracy: 0.0402 - val_loss: 4.7778 - val_accuracy: 0.0240\n",
      "Epoch 21/50\n",
      "215/215 [==============================] - 20s 94ms/step - loss: 4.4730 - accuracy: 0.0472 - val_loss: 4.8229 - val_accuracy: 0.0135\n",
      "Epoch 22/50\n",
      "215/215 [==============================] - 25s 118ms/step - loss: 4.4466 - accuracy: 0.0431 - val_loss: 4.7646 - val_accuracy: 0.0208\n",
      "Epoch 23/50\n",
      "215/215 [==============================] - 14s 66ms/step - loss: 4.4348 - accuracy: 0.0408 - val_loss: 4.7805 - val_accuracy: 0.0240\n",
      "Epoch 24/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 4.4234 - accuracy: 0.0490 - val_loss: 4.8227 - val_accuracy: 0.0188\n",
      "Epoch 25/50\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 4.4160 - accuracy: 0.0437 - val_loss: 4.7876 - val_accuracy: 0.0198\n",
      "Epoch 26/50\n",
      "215/215 [==============================] - 20s 92ms/step - loss: 4.4098 - accuracy: 0.0530 - val_loss: 4.7457 - val_accuracy: 0.0260\n",
      "Epoch 27/50\n",
      "215/215 [==============================] - 21s 96ms/step - loss: 4.4203 - accuracy: 0.0408 - val_loss: 4.7559 - val_accuracy: 0.0292\n",
      "Epoch 28/50\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 4.3886 - accuracy: 0.0425 - val_loss: 4.7536 - val_accuracy: 0.0240\n",
      "Epoch 29/50\n",
      "215/215 [==============================] - 22s 103ms/step - loss: 4.3591 - accuracy: 0.0565 - val_loss: 4.8088 - val_accuracy: 0.0198\n",
      "Epoch 30/50\n",
      "215/215 [==============================] - 25s 116ms/step - loss: 4.3742 - accuracy: 0.0513 - val_loss: 4.8400 - val_accuracy: 0.0250\n",
      "Epoch 31/50\n",
      "215/215 [==============================] - 24s 113ms/step - loss: 4.3638 - accuracy: 0.0455 - val_loss: 4.7993 - val_accuracy: 0.0281\n",
      "Epoch 32/50\n",
      "215/215 [==============================] - 28s 129ms/step - loss: 4.3402 - accuracy: 0.0530 - val_loss: 4.7794 - val_accuracy: 0.0229\n",
      "Epoch 33/50\n",
      "215/215 [==============================] - 24s 113ms/step - loss: 4.3298 - accuracy: 0.0565 - val_loss: 4.7739 - val_accuracy: 0.0260\n",
      "Epoch 34/50\n",
      "215/215 [==============================] - 24s 113ms/step - loss: 4.3238 - accuracy: 0.0565 - val_loss: 4.8374 - val_accuracy: 0.0250\n",
      "Epoch 35/50\n",
      "215/215 [==============================] - 20s 95ms/step - loss: 4.3392 - accuracy: 0.0507 - val_loss: 4.7958 - val_accuracy: 0.0281\n",
      "Epoch 36/50\n",
      "215/215 [==============================] - 16s 73ms/step - loss: 4.3085 - accuracy: 0.0600 - val_loss: 4.8678 - val_accuracy: 0.0281\n",
      "Epoch 37/50\n",
      "215/215 [==============================] - 28s 130ms/step - loss: 4.3079 - accuracy: 0.0664 - val_loss: 4.8222 - val_accuracy: 0.0281\n",
      "Epoch 38/50\n",
      "215/215 [==============================] - 25s 115ms/step - loss: 4.3013 - accuracy: 0.0513 - val_loss: 4.8098 - val_accuracy: 0.0312\n",
      "Epoch 39/50\n",
      "215/215 [==============================] - 14s 63ms/step - loss: 4.2928 - accuracy: 0.0670 - val_loss: 4.8550 - val_accuracy: 0.0240\n",
      "Epoch 40/50\n",
      "215/215 [==============================] - 17s 79ms/step - loss: 4.2918 - accuracy: 0.0571 - val_loss: 4.7772 - val_accuracy: 0.0292\n",
      "Epoch 41/50\n",
      "215/215 [==============================] - 14s 66ms/step - loss: 4.2797 - accuracy: 0.0653 - val_loss: 4.8295 - val_accuracy: 0.0312\n",
      "Epoch 42/50\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 4.2700 - accuracy: 0.0647 - val_loss: 4.8229 - val_accuracy: 0.0292\n",
      "Epoch 43/50\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 4.2812 - accuracy: 0.0635 - val_loss: 4.7995 - val_accuracy: 0.0344\n",
      "Epoch 44/50\n",
      "215/215 [==============================] - 14s 65ms/step - loss: 4.2548 - accuracy: 0.0734 - val_loss: 4.8474 - val_accuracy: 0.0344\n",
      "Epoch 45/50\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 4.2579 - accuracy: 0.0659 - val_loss: 4.8560 - val_accuracy: 0.0281\n",
      "Epoch 46/50\n",
      "215/215 [==============================] - 14s 63ms/step - loss: 4.2513 - accuracy: 0.0600 - val_loss: 4.7970 - val_accuracy: 0.0292\n",
      "Epoch 47/50\n",
      "215/215 [==============================] - 14s 63ms/step - loss: 4.2410 - accuracy: 0.0664 - val_loss: 4.8114 - val_accuracy: 0.0365\n",
      "Epoch 48/50\n",
      "215/215 [==============================] - 15s 71ms/step - loss: 4.2289 - accuracy: 0.0664 - val_loss: 4.8080 - val_accuracy: 0.0344\n",
      "Epoch 49/50\n",
      "215/215 [==============================] - 14s 65ms/step - loss: 4.2133 - accuracy: 0.0728 - val_loss: 4.8499 - val_accuracy: 0.0365\n",
      "Epoch 50/50\n",
      "215/215 [==============================] - 14s 65ms/step - loss: 4.2292 - accuracy: 0.0728 - val_loss: 4.8246 - val_accuracy: 0.0271\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 100 epochs\n",
    "model = tuner_inceptionv3.hypermodel.build(best_hps_inceptionv3)\n",
    "history_inceptionv3 = model.fit(train_datagen.flow(X, y, batch_size=8), epochs=50, validation_data=val_datagen.flow(x_val, y_val, batch_size=8), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deacdf3",
   "metadata": {},
   "source": [
    "## Sauvegarde du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c8f4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sauvegarde les différents estimateur\n",
    "\n",
    "history_inceptionv3.model.save('inceptionv3_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43e579d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv3 = keras.models.load_model('inceptionv3_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101165a4",
   "metadata": {},
   "source": [
    "## Test des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8514e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "test_datagen = ImageDataGenerator(preprocessing_function = tf.keras.applications.inception_v3.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a174702",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_inception = inceptionv3.predict(test_datagen.flow(x_test, y_test, batch_size=8, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "491d1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_inception = np.argmax(y_pred_inception, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26cfd2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04112028492795856"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred_inception,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
